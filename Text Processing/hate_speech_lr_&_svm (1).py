# -*- coding: utf-8 -*-
"""hate speech lr & svm

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Gh-xu8k2g_B1dWsJHdRYkYBGI8qO-tr-
"""

import pandas as pd
import numpy as np
import torch
import re
import tqdm
from matplotlib._path import (affine_transform, count_bboxes_overlapping_bbox, update_path_extents)

dataset = pd.read_csv('/content/dataset.csv')
dataset.dropna(inplace=True)
dataset

print("number of tweets belongin to classes 0,1 and 2")
dataset.groupby('class')['id'].nunique()

dataset.groupby('class')['id'].nunique().plot(kind='bar', title='Plot of number of tweets belonging to a particular class')

dataset0 = dataset[dataset['class'] == 0]
dataset1 = dataset[dataset['class'] == 1]
dataset2 = dataset[dataset['class'] == 2]

count0 = dataset0.shape[0]
count1 = dataset1.shape[0]
count2 = dataset2.shape[0]
print(count0, count1, count2)

dataset1_under = dataset1.sample(count2)
dataset = pd.concat([dataset0, dataset1_under, dataset2], axis = 0)
dataset.head()

print("number of tweets belonging to classes 0,1 and 2")
dataset.groupby('class')['id'].nunique()

dataset.groupby('class')['id'].nunique().plot(kind='bar',title='Plot of number of tweets belonging to a particular class')

!pip install gensim
from nltk import tokenize
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

from gensim.models import Word2Vec

from sklearn.model_selection import train_test_split

import nltk
nltk.download('stopwords')

stop_words= set(stopwords.words('english'))

import nltk
nltk.download('punkt')
nltk.download('punkt_tab')

def clean_tweet(tweet):
    tweet = re.sub("#", "",tweet) # Removing '#' from hashtags
    tweet = re.sub("[^a-zA-Z#]", " ",tweet) # Removing punctuation and special characters
    tweet = re.sub(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\(\),]|(?:%[0-9a-f][0-9a-f]))+',"<URL>", tweet)
    tweet = re.sub('http','',tweet)
    tweet = re.sub(" +", " ", tweet)
    tweet = tweet.lower()
    tweet = word_tokenize(tweet)
    return_tweet=[]
    for word in tweet:
        if word not in stop_words:
            return_tweet.append(word)
    return return_tweet
dataset["tweet"]=dataset["tweet"].apply(clean_tweet)

9from sklearn.feature_extraction.text import TfidfVectorizer

# Convert tokens back to text (TF-IDF expects strings)
dataset["tweet_text"] = dataset["tweet"].apply(lambda x: " ".join(x))

tfidf = TfidfVectorizer(
    min_df=3,
    max_df=0.90,
    sublinear_tf=True,
    norm='l2',
    ngram_range=(1,3),
    analyzer='word'
)

X = tfidf.fit_transform(dataset["tweet_text"])

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y = le.fit_transform(dataset["class"])

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

from sklearn.linear_model import LogisticRegression

lr = LogisticRegression(
    C=2,
    max_iter=5000,
    class_weight='balanced',
    solver='lbfgs',
    multi_class='multinomial'
)

lr.fit(X_train, y_train)
y_pred_lr = lr.predict(X_test)

from sklearn.svm import LinearSVC

svm_clf = LinearSVC(
    C=1.5,
    class_weight='balanced'
)

svm_clf.fit(X_train, y_train)
y_pred_svm = svm_clf.predict(X_test)

from sklearn.metrics import classification_report, accuracy_score, f1_score

print("LR Accuracy:", accuracy_score(y_test, y_pred_lr))
print("LR F1:", f1_score(y_test, y_pred_lr, average='macro'))
print("\nSVM Accuracy:", accuracy_score(y_test, y_pred_svm))
print("SVM F1:", f1_score(y_test, y_pred_svm, average='macro'))

from sklearn.model_selection import GridSearchCV

param_grid = {
    'C': [0.5, 1, 2, 5]
}

grid = GridSearchCV(LinearSVC(class_weight='balanced'), param_grid, cv=3, scoring='f1_macro')
grid.fit(X_train, y_train)

best_svm = grid.best_estimator_
y_pred_best = best_svm.predict(X_test)

print("Best SVM Accuracy:", accuracy_score(y_test, y_pred_best))
print("Best SVM F1:", f1_score(y_test, y_pred_best, average='macro'))

"""xxxxxx"""

#model = Word2Vec(dataset["tweet"].values, vector_size=50, window=5, min_count=1, workers=4)

"""def get_features(tweet):
    features=[]
    for word in tweet:
        if word in model.wv.key_to_index: # Check if word exists in model's vocabulary
            features.append(model.wv[word])
    if features: # Return mean of features if any were found
        return np.mean(features,0)
    else:
        # Return a zero vector of the same dimension if no words were found in the vocabulary
        return np.zeros(model.vector_size)"""

dataset["features"]=dataset["tweet"].apply(get_features)

data=[]
for i in dataset["features"].values:
    temp=[]
    for j in i:
        temp.append(j)
    data.append(temp)
data=np.array(data)

from sklearn.preprocessing import label_binarize

Y = label_binarize(dataset["class"].values, classes=[0, 1, 2])
n_classes = Y.shape[1]
X_train, X_test, y_train, y_test = train_test_split(data, Y, test_size=0.2, random_state=42)

print(X_train)
print(y_train)

from sklearn.linear_model import LogisticRegression
from sklearn.multiclass import OneVsRestClassifier
from sklearn import svm
from sklearn.metrics import f1_score
from sklearn.metrics import recall_score
from sklearn.metrics import precision_score
from sklearn.metrics import precision_recall_curve
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

lr_clf = OneVsRestClassifier(LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'))
lr_clf.fit(X_train,y_train)
y_pred = lr_clf.predict(X_test)
f = f1_score(y_test, y_pred, average='micro')
print("F1 Score: ", f)
p = precision_score(y_test, y_pred, average='micro')
print("Precision Score: ", p)
r = recall_score(y_test, y_pred, average='micro')
print("Recall Score: ", r)
print("Accuracy: ", lr_clf.score(X_test,y_test))

y_score = lr_clf.predict_proba(X_test)
precision = dict()
recall = dict()
for i in range(n_classes):
    precision[i], recall[i], _ = precision_recall_curve(y_test[:, i],
                                                        y_score[:, i])
    plt.plot(recall[i], precision[i], lw=2, label='class {}'.format(i))

plt.xlabel("Recall")
plt.ylabel("Precision")
plt.legend(loc = "best")
plt.title("Precision vs. Recall curve")
plt.show()

svm_clf = OneVsRestClassifier(svm.SVC(gamma='scale', probability=True))
svm_clf.fit(X_train,y_train)
y_pred = svm_clf.predict(X_test)
f = f1_score(y_test, y_pred, average='micro')
print("F1 Score: ", f)
p = precision_score(y_test, y_pred, average='micro')
print("Precision Score: ", p)
r = recall_score(y_test, y_pred, average='micro')
print("Recall Score: ", r)
print("Accuracy: ", svm_clf.score(X_test,y_test))

y_score = svm_clf.predict_proba(X_test)
precision = dict()
recall = dict()
for i in range(n_classes):
    precision[i], recall[i], _ = precision_recall_curve(y_test[:, i],
                                                        y_score[:, i])
    plt.plot(recall[i], precision[i], lw=2, label='class {}'.format(i))

plt.xlabel("Recall")
plt.ylabel("Precision")
plt.legend(loc = "center right")
plt.title("Precision vs. Recall curve")
plt.show()

user_input_text = input("Enter the text you want to classify: ")

cleaned_user_text = clean_tweet(user_input_text)

# Handle cases where the cleaned text might be empty (e.g., only stop words or special characters)
if not cleaned_user_text:
    print("No meaningful words found after cleaning the input. Cannot classify.")
else:
    user_features = get_features(cleaned_user_text)
    # Reshape the features to be a 2D array as expected by the classifier
    user_features_reshaped = user_features.reshape(1, -1)

    # Predict the class using the Logistic Regression classifier
    predicted_class_one_hot = lr_clf.predict(user_features_reshaped)

    # Convert one-hot encoded prediction back to a single class label
    predicted_class_label = np.argmax(predicted_class_one_hot)

    print(f"\nInput Text: '{user_input_text}'")
    print(f"Predicted Class: {predicted_class_label}")

user_input_text = input("Enter the text you want to classify: ")

cleaned_user_text = clean_tweet(user_input_text)

# Handle cases where the cleaned text might be empty (e.g., only stop words or special characters)
if not cleaned_user_text:
    print("No meaningful words found after cleaning the input. Cannot classify.")
else:
    user_features = get_features(cleaned_user_text)
    # Reshape the features to be a 2D array as expected by the classifier
    user_features_reshaped = user_features.reshape(1, -1)

    # Predict the class using the Logistic Regression classifier
    predicted_class_one_hot = lr_clf.predict(user_features_reshaped)

    # Convert one-hot encoded prediction back to a single class label
    predicted_class_label = np.argmax(predicted_class_one_hot)

    print(f"\nInput Text: '{user_input_text}'")
    print(f"Predicted Class: {predicted_class_label}")